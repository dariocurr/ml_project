{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tracks_recommendation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5-final"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0qSHcf2c4-5"
      },
      "source": [
        "# Deezer playlist dataset and song recommendation with word2vec\n",
        "\n",
        "In this mini project we will develop a word2vec network and use it to build a playlist completion tool (song suggestion). The data is hosted on the following repository: http://github.com/comeetie/deezerplay.git. To know more about word2vec and these data you can read the two following references:\n",
        "\n",
        "- Efficient estimation of word representations in vector space, Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. (https://arxiv.org/abs/1301.3781)\n",
        "- Word2with applied to Recommendation: Hyperparameters Matter, H. Caselles-DuprÃ©, F. Lesaint and J. Royo-Letelier. (https://arxiv.org/pdf/1804.04212.pdf)\n",
        "\n",
        "The elements you have to do are highlighted in red."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDhFaGLDc4_G"
      },
      "source": [
        "## Preparation of data\n",
        "\n",
        "The data is in the form of a playlist list. Each playlist is a list with the deezer ID of the psong followed by the artist ID."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SQRBK18Ec4_H",
        "outputId": "6e5da44e-7198-452f-d7e7-5267d5d97e9f"
      },
      "source": [
        "import numpy as np\n",
        "data = np.load(\"data/music_2.npy\", allow_pickle=True)\n",
        "\n",
        "# info visualization\n",
        "print(\"number of playlists: \" + str(len(data)))\n",
        "print(\"average number of songs in a playlist: \" + str(np.mean([len(p) for p in data])))\n",
        "print(\"\\nfirst row of dataset:\\n\" + str(data[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of playlists: 100000\n",
            "average number of songs in a playlist: 24.21338\n",
            "\n",
            "first row of dataset:\n",
            "['track_3248376', 'artist_4660', 'track_68116150', 'artist_210', 'track_68116150', 'artist_210', 'track_3169189', 'artist_7188', 'track_6523608', 'artist_2961', 'track_407020492', 'artist_2961', 'track_6523613', 'artist_2961', 'track_348627211', 'artist_396485', 'track_348627221', 'artist_396485', 'track_348627231', 'artist_396485', 'track_348627241', 'artist_396485', 'track_348627251', 'artist_2961', 'track_348627261', 'artist_2961', 'track_348627271', 'artist_2961', 'track_348627281', 'artist_2961', 'track_348627291', 'artist_2961']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZTP4KFqc4_J"
      },
      "source": [
        "The dataset we are going to work on contains 100000 playlists which are composed of an average of 24.1 songs. We will start by keeping only the song identifiers. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5fQlCaG6c4_J",
        "outputId": "e67e34b7-f124-4fe2-f346-6bd1ff1a2f9e"
      },
      "source": [
        "playlist_track = [list(filter(lambda w: w.split(\"_\")[0] == u\"track\", playlist)) for playlist in data]\n",
        "playlist_artist = [list(filter(lambda w: w.split(\"_\")[0] == u\"artist\", playlist)) for playlist in data]\n",
        "\n",
        "# info visualization\n",
        "print(\"first row of playlist_track dataset:\\n\" + str(playlist_track[0]))\n",
        "print(\"\\nfirst row of playlist_artist dataset:\\n\" + str(playlist_artist[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "first row of playlist_track dataset:\n",
            "['track_3248376', 'track_68116150', 'track_68116150', 'track_3169189', 'track_6523608', 'track_407020492', 'track_6523613', 'track_348627211', 'track_348627221', 'track_348627231', 'track_348627241', 'track_348627251', 'track_348627261', 'track_348627271', 'track_348627281', 'track_348627291']\n",
            "\n",
            "first row of playlist_artist dataset:\n",
            "['artist_4660', 'artist_210', 'artist_210', 'artist_7188', 'artist_2961', 'artist_2961', 'artist_2961', 'artist_396485', 'artist_396485', 'artist_396485', 'artist_396485', 'artist_2961', 'artist_2961', 'artist_2961', 'artist_2961', 'artist_2961']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i2VNXU-Oc4_K",
        "outputId": "b2fb5e03-fa08-4670-eccf-7e75b6987ac8"
      },
      "source": [
        "# songs != playlists\n",
        "tracks = np.unique(np.concatenate(playlist_track))\n",
        "Vt = len(tracks)\n",
        "\n",
        "# info visualization\n",
        "print(\"number of tracks: \" + str(Vt))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of tracks: 338509\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGvx4LmIc4_K"
      },
      "source": [
        "The number of different songs in this data-set is quite high with more than 300,000 songs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vM_K2FLfc4_L"
      },
      "source": [
        "## Creating a song dictionary\n",
        "We will assign to each song an integer that will serve as a unique identifier and input for our network. In order to save a little bit of resources we will only work in this project on songs that appear in at least two playlists."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oUngLEkjc4_L",
        "outputId": "b082b55e-6d8e-4eb3-e7ec-6cb4f674e624"
      },
      "source": [
        "# counting occurences for each track\n",
        "track_counts = dict((tracks[i], 0) for i in range(0, Vt))\n",
        "for p in playlist_track:\n",
        "    for track in p:\n",
        "        track_counts[track] = track_counts[track] + 1\n",
        "\n",
        "# info visualization\n",
        "print(\"first 10 rows of track_counts dictionary:\")\n",
        "for key in list(track_counts.keys())[:10]:\n",
        "    print(str(key) + \": \" + str(track_counts[key]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "first 10 rows of track_counts dictionary:\n",
            "track_100001352: 1\n",
            "track_100001490: 1\n",
            "track_100001878: 1\n",
            "track_100001884: 12\n",
            "track_100004586: 16\n",
            "track_100004588: 14\n",
            "track_100004590: 219\n",
            "track_100004592: 13\n",
            "track_100004594: 13\n",
            "track_100004596: 10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1kU8oUefc4_M",
        "outputId": "80ee1999-607c-436c-e8c9-7c40cbda9725"
      },
      "source": [
        "# Filter very rare songs to save resources\n",
        "playlist_track_filter = [list(filter(lambda track : track_counts[track] > 1, playlist)) for playlist in playlist_track]\n",
        "# get the counts\n",
        "counts = np.array(list(track_counts.values()))\n",
        "# sort\n",
        "order = np.argsort(-counts)\n",
        "# deezed_id array\n",
        "tracks_list_ordered = np.array(list(track_counts.keys()))[order]\n",
        "# Vocabulary size = number of kept songs\n",
        "Vt = np.where(counts[order] == 1)[0][0]         # or Vt = sum([1 for count in counts if count != 1])\n",
        "# dict construction id_morceaux num_id [0,Vt]\n",
        "track_dict = dict((tracks_list_ordered[i], i) for i in range(0, Vt))\n",
        "# playlist conversion to list of integers\n",
        "corpus_num_track = [[track_dict[track] for track in play] for play in playlist_track_filter]\n",
        "\n",
        "\n",
        "# info visualization\n",
        "print(\"first row of playlist_track_filter dataset:\\n\" + str(playlist_track_filter[0]))\n",
        "print(\"\\ncounts: \" + str(counts))\n",
        "print(\"indexes which sort the counts array in descending order: \" + str(order))\n",
        "print(\"\\nsorted track_list:\\n\" + str(tracks_list_ordered))\n",
        "print(\"\\nnumber of tracks that appear at least two times: \" + str(Vt))\n",
        "print(\"\\nfirst 10 rows of track dictionary:\")\n",
        "for key in list(track_dict.keys())[:10]:\n",
        "    print(str(key) + \": \" + str(track_dict[key]))\n",
        "print(\"\\nfirst row of corpus_num_track dataset: \" + str(corpus_num_track[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "first row of playlist_track_filter dataset:\n",
            "['track_3248376', 'track_68116150', 'track_68116150', 'track_3169189', 'track_6523608', 'track_407020492', 'track_6523613']\n",
            "\n",
            "counts: [1 1 1 ... 1 1 1]\n",
            "indexes which sort the counts array in descending order: [193287 194559 106371 ... 135378 135361 338508]\n",
            "\n",
            "sorted track_list:\n",
            "['track_380684541' 'track_382428781' 'track_139470659' ...\n",
            " 'track_16514878' 'track_16504678' 'track_999941']\n",
            "\n",
            "number of tracks that appear at least two times: 123241\n",
            "\n",
            "first 10 rows of track dictionary:\n",
            "track_380684541: 0\n",
            "track_382428781: 1\n",
            "track_139470659: 2\n",
            "track_402932972: 3\n",
            "track_375689861: 4\n",
            "track_398649632: 5\n",
            "track_403074632: 6\n",
            "track_402932922: 7\n",
            "track_362795841: 8\n",
            "track_375437431: 9\n",
            "\n",
            "first row of corpus_num_track dataset: [17104, 13945, 13945, 13845, 19340, 79029, 23294]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2m_TBN_Tc4_M"
      },
      "source": [
        "### Creation of test and validation learning sets\n",
        "\n",
        "To learn the parameters of our method we will keep the first l-1 songs of each playlist (with l the length of the playlist) for learning. To evaluate the completion performance of our method we keep for each playlist the last two songs. The objective will be to find the last one from the next-to-last one. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UyxciJAqc4_N",
        "outputId": "e15120ff-e85b-4603-d19b-c9a4be31a35c"
      },
      "source": [
        "# playlist main part used for training\n",
        "play_app  = [corpus_num_track[i][:(len(corpus_num_track[i])-1)] for i in range(len(corpus_num_track)) if len(corpus_num_track[i]) > 1]      # or play_app = [playlist[:len(playlist) - 1] for playlist in corpus_num_track if len(playlist) > 1]\n",
        "\n",
        "# the two last elements are used for validation and training\n",
        "index_tst = np.random.choice(100000, 20000)\n",
        "index_val = np.setdiff1d(range(100000), index_tst)\n",
        "\n",
        "play_tst  = np.array([corpus_num_track[i][(len(corpus_num_track[i]) - 2):len(corpus_num_track[i])] \n",
        "             for i in index_tst if len(corpus_num_track[i]) > 3])\n",
        "play_val  = np.array([corpus_num_track[i][(len(corpus_num_track[i]) - 2):len(corpus_num_track[i])] \n",
        "             for i in index_val if len(corpus_num_track[i]) > 3])[:10000]\n",
        "\n",
        "\n",
        "# info visualization\n",
        "print(\"first row of play_app dataset: \" + str(play_app[0]))\n",
        "print(\"20000 test indexes less than 100000: \" + str(index_tst))\n",
        "print(\"int not contained in index_tst: \" + str(index_val))\n",
        "print(\"\\nfirst 3 test pairs:\")\n",
        "for pair in play_tst[:3]:\n",
        "    print(pair)\n",
        "print(\"\\nfirst 3 validation pairs:\")\n",
        "for pair in play_val[:3]:\n",
        "    print(pair)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "first row of play_app dataset: [17104, 13945, 13945, 13845, 19340, 79029]\n",
            "20000 test indexes less than 100000: [13986 34801 62511 ... 26738 31848 84564]\n",
            "int not contained in index_tst: [    0     1     2 ... 99996 99997 99999]\n",
            "\n",
            "first 3 test pairs:\n",
            "[ 163 1363]\n",
            "[41116 92988]\n",
            "[17166 21675]\n",
            "\n",
            "first 3 validation pairs:\n",
            "[79029 23294]\n",
            "[ 532 2016]\n",
            "[50537 33361]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vwy-EnmIc4_O"
      },
      "source": [
        "# import Keras\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Embedding, Reshape, Activation, Input, Dense, Flatten\n",
        "from keras.layers.merge import Dot\n",
        "from keras.utils import np_utils\n",
        "from keras.preprocessing.sequence import skipgrams"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkdXsyTac4_O"
      },
      "source": [
        "### hyper-parameters of word2vec :\n",
        "\n",
        "the method word2vec needs some hyper-parameters. We are going to give them the first values, but we will refine them later:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nYUQ0F7Wc4_P"
      },
      "source": [
        "# latent space dimension\n",
        "vector_dim = 30\n",
        "# window size\n",
        "window_width = 3\n",
        "# number of negative sample per positive sample\n",
        "neg_sample = 5\n",
        "# size o mini-batch\n",
        "min_batch_size = 50\n",
        "# smoothing factor for the sampling table of negative pairs \n",
        "samp_coef = 0.5\n",
        "# cparameter to sub-sample frequent song\n",
        "sub_samp = 0.00001"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kdxE_-7c4_Q"
      },
      "source": [
        "### Creation of the draw probability tables (smoothed) and unsmoothed\n",
        "\n",
        "To draw the negative examples we need the smoothed frequencies of each song in our dataset. Likewise to under-sample very frequent pieces we need the raw frequencies. We will calculate these two vectors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5EquUawxc4_Q",
        "outputId": "023669d9-3e76-4457-c029-434b7a457a53"
      },
      "source": [
        "# get the counts\n",
        "counts = np.array(list(track_counts.values()), dtype='float')[order[:Vt]]\n",
        "# normalization\n",
        "st = counts/np.sum(counts)\n",
        "# smoothing\n",
        "st_smooth = np.power(st, samp_coef)\n",
        "st_smooth = st_smooth / np.sum(st_smooth)\n",
        "\n",
        "# info visualization\n",
        "print(\"counts of tracks which appear at least two times: \" + str(counts))\n",
        "print(\"\\nfrequencies of tracks which appear at least two times:\\n\" + str(st))\n",
        "print(\"\\nsmoothed frequencies of tracks which appear at least two times:\\n\" + str(st_smooth))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "counts of tracks which appear at least two times: [1898. 1805. 1673. ...    2.    2.    2.]\n",
            "\n",
            "frequencies of tracks which appear at least two times:\n",
            "[1.90676923e-03 1.81333955e-03 1.68072968e-03 ... 2.00924050e-06\n",
            " 2.00924050e-06 2.00924050e-06]\n",
            "\n",
            "smoothed frequencies of tracks which appear at least two times:\n",
            "[1.56339799e-04 1.52461451e-04 1.46780856e-04 ... 5.07500463e-06\n",
            " 5.07500463e-06 5.07500463e-06]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0M24kdtfc4_R"
      },
      "source": [
        "### Building the word2 network with\n",
        "\n",
        "A word2 network with takes in input two integers corresponding to two songs, these are embedded in a latent space of dimension (vector_dim) thanks to a layer of embedding type (you will have to use the same layer to project the two pieces). Once these two vectors have been extracted, the array must calculate their scalar product normalize appleler cosine distance : \n",
        "\n",
        "$$cos(\\theta_{ij})=\\frac{z_i.z_j}{||z_i||||z_j||}$$\n",
        "\n",
        "To carry out this treatment you will use a \"dot\" layer for \"dot product\". The model then uses a sigmoid layer to produce the output. This output will be 0 when both songs are randomly drawn from the whole dataset and 1 when they were extracted from the same playslist. <span style=\"color:red\">You have to create the keras Track2Vec model corresponding to this architecture.</span>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pikU9pPLc4_R"
      },
      "source": [
        "# Two inputs: the target song and a real or negative context song\n",
        "input_target = Input((1,), dtype='int32')\n",
        "input_context = Input((1,), dtype='int32')\n",
        "\n",
        "# Embedding try to compress large one-hot song vectors into much smaller vectors which preserve some of the      # meaning and context of the word\n",
        "embedding = Embedding(Vt, vector_dim, input_length=1, name='embedding')\n",
        "target = embedding(input_target)\n",
        "context = embedding(input_context)\n",
        "\n",
        "# The dot product is used to get the real similarity\n",
        "dot_product = Dot(axes=2)([target, context])\n",
        "flatten = Flatten()(dot_product)\n",
        "\n",
        "# Output the similarity to a sigmoid layer to give us a 1 or 0 indicator which we can match with the label given # to the Context word (1 for a true context word, 0 for a negative sample).\n",
        "output = Dense(1, activation='sigmoid',name=\"classif\")(flatten)\n",
        "\n",
        "Track2Vec = Model(inputs=[input_target, input_context], outputs=output)\n",
        "Track2Vec.compile(loss='binary_crossentropy', optimizer='adam', metrics=[\"accuracy\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qm5l_K6Fc4_S",
        "outputId": "1c94ce76-af89-45f1-b272-b56ab591f300"
      },
      "source": [
        "Track2Vec.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, 1, 30)        3697230     input_1[0][0]                    \n",
            "                                                                 input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dot (Dot)                       (None, 1, 1)         0           embedding[0][0]                  \n",
            "                                                                 embedding[1][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "flatten (Flatten)               (None, 1)            0           dot[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "classif (Dense)                 (None, 1)            2           flatten[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 3,697,232\n",
            "Trainable params: 3,697,232\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79vbsexuc4_T"
      },
      "source": [
        "### Creation of the data generator\n",
        "\n",
        "To learn the projection layer at the heart of our model we will build a generator of positive and negative pair examples of close or random songs from our training data. The following function will allow us to generate such examples from a playlist (seq) provided as input. This function will first build all the pairs of songs that can be extracted from the playlist if they are within (windows) distance of each other. These pairs will constitute the positive pairs. The pairs concerning very frequent songs will be removed with a probability that depends on their frequencies. Finally a number of negative examples (corresponding to neg_samples * positive number of examples) will be randomly drawn using the neg_sampling_table."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gt-jof2pc4_T"
      },
      "source": [
        "# function to generate word2vec positive and negative pairs \n",
        "# from an array of int that represent a text ot here a playlist\n",
        "# params \n",
        "# seq : input text or playlist (array of int)\n",
        "# neg_samples : number of negative sample to generate per positive ones\n",
        "# neg_sampling_table : sampling table for negative samples\n",
        "# sub sampling_table : sampling table for sub sampling common words songs\n",
        "# sub_t : sub sampling parameter\n",
        "def word2vecSampling(seq, window, neg_samples, neg_sampling_table, sub_sampling_table, sub_t):\n",
        "    # vocab size\n",
        "    V = len(neg_sampling_table)\n",
        "    # extract positive pairs \n",
        "    positives = skipgrams(sequence=seq, vocabulary_size=V, window_size=window, negative_samples=0)\n",
        "    ppairs    = np.array(positives[0])\n",
        "    # sub sampling\n",
        "    if (ppairs.shape[0]>0):\n",
        "        f = sub_sampling_table[ppairs[:,0]]\n",
        "        subprob = ((f-sub_t)/f)-np.sqrt(sub_t/f)\n",
        "        tokeep = (subprob<np.random.uniform(size=subprob.shape[0])) | (subprob<0)\n",
        "        ppairs = ppairs[tokeep,:]\n",
        "    nbneg     = ppairs.shape[0]*neg_samples\n",
        "    # sample negative pairs\n",
        "    if (nbneg > 0):\n",
        "        negex     = np.random.choice(V, nbneg, p=neg_sampling_table)\n",
        "        negexcontext = np.repeat(ppairs[:,0],neg_samples)\n",
        "        npairs    = np.transpose(np.stack([negexcontext,negex]))\n",
        "        pairs     = np.concatenate([ppairs,npairs],axis=0)\n",
        "        labels    = np.concatenate([np.repeat(1,ppairs.shape[0]),np.repeat(0,nbneg)])\n",
        "        perm      = np.random.permutation(len(labels))\n",
        "        res = [pairs[perm,:],labels[perm]]\n",
        "    else:\n",
        "        res=[[],[]]\n",
        "    return res"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDrj3fzXc4_U"
      },
      "source": [
        "<span style=\"color:red\">Use this function to build a \"track_ns_generator\" of data which will generate positive and negative examples from \"nbm\" playlists randomly drawn from the \"corpus_num\" dataset provided as input. </span> "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4HtIoYiFc4_U"
      },
      "source": [
        "import random\n",
        "\n",
        "def track_ns_generator(corpus_num, nbm):\n",
        "    while 1:\n",
        "        # Extraction of nbm playlist from corpus_num\n",
        "        playlists = [corpus_num[random.randint(0, len(corpus_num) - 1)] for _ in range(nbm)]\n",
        "        # Creation of x and y \n",
        "        x = np.ndarray((0, 2), dtype=np.int32)\n",
        "        y = np.ndarray((0), dtype=np.int32)\n",
        "        for playlist in playlists:\n",
        "            # For each playlist we use the word2vecSampling function to get the couple of negative or real                   # similar songs (sx) and the labels, 1 for real similar songs and 0 for negative samples (sy) \n",
        "            sx, sy = word2vecSampling(playlist, window_width, neg_sample, st_smooth, st, sub_samp)\n",
        "\n",
        "            # Check if sx is not empty\n",
        "            if len(sx) > 0:\n",
        "                x = np.vstack((x, sx))\n",
        "                y = np.append(y, sy)\n",
        "        \n",
        "        # Generate the results\n",
        "        yield ((x[:, 0], x[:, 1]), y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zIr6UHGc4_V"
      },
      "source": [
        "## Learning \n",
        "You should now be able to learn your first model with the following code. This should take between 15 and 30 min."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "4RavK6gyc4_V",
        "outputId": "331727c8-7889-42f0-f7e9-deded0ef2e35"
      },
      "source": [
        "hist = Track2Vec.fit(track_ns_generator(play_app, min_batch_size), steps_per_epoch=200, epochs=60)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/60\n",
            "200/200 [==============================] - 27s 133ms/step - loss: 0.6773 - accuracy: 0.8231\n",
            "Epoch 2/60\n",
            "200/200 [==============================] - 27s 133ms/step - loss: 0.6207 - accuracy: 0.8333\n",
            "Epoch 3/60\n",
            "200/200 [==============================] - 26s 132ms/step - loss: 0.5764 - accuracy: 0.8333\n",
            "Epoch 4/60\n",
            "200/200 [==============================] - 26s 130ms/step - loss: 0.5418 - accuracy: 0.8334\n",
            "Epoch 5/60\n",
            "200/200 [==============================] - 26s 131ms/step - loss: 0.5139 - accuracy: 0.8340\n",
            "Epoch 6/60\n",
            "200/200 [==============================] - 27s 137ms/step - loss: 0.4890 - accuracy: 0.8366\n",
            "Epoch 7/60\n",
            "200/200 [==============================] - 28s 139ms/step - loss: 0.4674 - accuracy: 0.8398\n",
            "Epoch 8/60\n",
            "200/200 [==============================] - 27s 132ms/step - loss: 0.4474 - accuracy: 0.8436\n",
            "Epoch 9/60\n",
            "200/200 [==============================] - 26s 131ms/step - loss: 0.4292 - accuracy: 0.8480\n",
            "Epoch 10/60\n",
            "200/200 [==============================] - 26s 132ms/step - loss: 0.4156 - accuracy: 0.8503\n",
            "Epoch 11/60\n",
            "200/200 [==============================] - 27s 133ms/step - loss: 0.4000 - accuracy: 0.8549\n",
            "Epoch 12/60\n",
            "200/200 [==============================] - 26s 132ms/step - loss: 0.3869 - accuracy: 0.8583\n",
            "Epoch 13/60\n",
            "200/200 [==============================] - 26s 131ms/step - loss: 0.3741 - accuracy: 0.8614\n",
            "Epoch 14/60\n",
            "200/200 [==============================] - 27s 133ms/step - loss: 0.3629 - accuracy: 0.8644\n",
            "Epoch 15/60\n",
            "200/200 [==============================] - 27s 133ms/step - loss: 0.3421 - accuracy: 0.8720\n",
            "Epoch 16/60\n",
            "200/200 [==============================] - 26s 131ms/step - loss: 0.3331 - accuracy: 0.8738\n",
            "Epoch 17/60\n",
            "200/200 [==============================] - 26s 131ms/step - loss: 0.3132 - accuracy: 0.8821\n",
            "Epoch 18/60\n",
            "200/200 [==============================] - 27s 133ms/step - loss: 0.3029 - accuracy: 0.8857\n",
            "Epoch 19/60\n",
            "200/200 [==============================] - 27s 136ms/step - loss: 0.2969 - accuracy: 0.8879\n",
            "Epoch 20/60\n",
            "200/200 [==============================] - 27s 135ms/step - loss: 0.2837 - accuracy: 0.8937\n",
            "Epoch 21/60\n",
            "200/200 [==============================] - 28s 139ms/step - loss: 0.2783 - accuracy: 0.8960\n",
            "Epoch 22/60\n",
            "200/200 [==============================] - 29s 145ms/step - loss: 0.2713 - accuracy: 0.8975\n",
            "Epoch 23/60\n",
            "200/200 [==============================] - 27s 135ms/step - loss: 0.2629 - accuracy: 0.9010\n",
            "Epoch 24/60\n",
            "200/200 [==============================] - 26s 132ms/step - loss: 0.2557 - accuracy: 0.9053\n",
            "Epoch 25/60\n",
            "200/200 [==============================] - 27s 135ms/step - loss: 0.2496 - accuracy: 0.9073\n",
            "Epoch 26/60\n",
            "200/200 [==============================] - 28s 141ms/step - loss: 0.2376 - accuracy: 0.9125\n",
            "Epoch 27/60\n",
            "200/200 [==============================] - 27s 136ms/step - loss: 0.2283 - accuracy: 0.9159\n",
            "Epoch 28/60\n",
            "200/200 [==============================] - 28s 139ms/step - loss: 0.2188 - accuracy: 0.9195\n",
            "Epoch 29/60\n",
            "200/200 [==============================] - 28s 140ms/step - loss: 0.2099 - accuracy: 0.9238\n",
            "Epoch 30/60\n",
            "200/200 [==============================] - 28s 138ms/step - loss: 0.2049 - accuracy: 0.9257\n",
            "Epoch 31/60\n",
            "200/200 [==============================] - 27s 133ms/step - loss: 0.2069 - accuracy: 0.9237\n",
            "Epoch 32/60\n",
            "200/200 [==============================] - 28s 138ms/step - loss: 0.2031 - accuracy: 0.9254\n",
            "Epoch 33/60\n",
            "200/200 [==============================] - 28s 138ms/step - loss: 0.1976 - accuracy: 0.9280\n",
            "Epoch 34/60\n",
            "200/200 [==============================] - 28s 141ms/step - loss: 0.1919 - accuracy: 0.9302\n",
            "Epoch 35/60\n",
            "200/200 [==============================] - 27s 136ms/step - loss: 0.1928 - accuracy: 0.9292\n",
            "Epoch 36/60\n",
            "200/200 [==============================] - 27s 135ms/step - loss: 0.1889 - accuracy: 0.9308\n",
            "Epoch 37/60\n",
            "200/200 [==============================] - 27s 135ms/step - loss: 0.1823 - accuracy: 0.9353\n",
            "Epoch 38/60\n",
            "200/200 [==============================] - 27s 133ms/step - loss: 0.1723 - accuracy: 0.9385\n",
            "Epoch 39/60\n",
            "200/200 [==============================] - 26s 132ms/step - loss: 0.1697 - accuracy: 0.9401\n",
            "Epoch 40/60\n",
            "200/200 [==============================] - 27s 133ms/step - loss: 0.1631 - accuracy: 0.9426\n",
            "Epoch 41/60\n",
            "200/200 [==============================] - 28s 138ms/step - loss: 0.1607 - accuracy: 0.9432\n",
            "Epoch 42/60\n",
            "200/200 [==============================] - 27s 135ms/step - loss: 0.1618 - accuracy: 0.9424\n",
            "Epoch 43/60\n",
            "200/200 [==============================] - 27s 136ms/step - loss: 0.1538 - accuracy: 0.9459\n",
            "Epoch 44/60\n",
            "200/200 [==============================] - 27s 134ms/step - loss: 0.1533 - accuracy: 0.9454\n",
            "Epoch 45/60\n",
            "200/200 [==============================] - 27s 133ms/step - loss: 0.1548 - accuracy: 0.9446\n",
            "Epoch 46/60\n",
            "200/200 [==============================] - 27s 136ms/step - loss: 0.1488 - accuracy: 0.9472\n",
            "Epoch 47/60\n",
            "200/200 [==============================] - 27s 135ms/step - loss: 0.1539 - accuracy: 0.9447\n",
            "Epoch 48/60\n",
            "200/200 [==============================] - 27s 136ms/step - loss: 0.1464 - accuracy: 0.9482\n",
            "Epoch 49/60\n",
            "200/200 [==============================] - 28s 138ms/step - loss: 0.1495 - accuracy: 0.9466\n",
            "Epoch 50/60\n",
            "200/200 [==============================] - 27s 136ms/step - loss: 0.1393 - accuracy: 0.9503\n",
            "Epoch 51/60\n",
            "200/200 [==============================] - 26s 132ms/step - loss: 0.1351 - accuracy: 0.9519\n",
            "Epoch 52/60\n",
            "200/200 [==============================] - 27s 135ms/step - loss: 0.1330 - accuracy: 0.9518\n",
            "Epoch 53/60\n",
            "200/200 [==============================] - 27s 133ms/step - loss: 0.1285 - accuracy: 0.9543\n",
            "Epoch 54/60\n",
            "200/200 [==============================] - 27s 136ms/step - loss: 0.1350 - accuracy: 0.9506\n",
            "Epoch 55/60\n",
            "200/200 [==============================] - 27s 134ms/step - loss: 0.1283 - accuracy: 0.9538\n",
            "Epoch 56/60\n",
            "200/200 [==============================] - 27s 136ms/step - loss: 0.1341 - accuracy: 0.9509\n",
            "Epoch 57/60\n",
            "200/200 [==============================] - 27s 136ms/step - loss: 0.1271 - accuracy: 0.9538\n",
            "Epoch 58/60\n",
            "200/200 [==============================] - 26s 132ms/step - loss: 0.1275 - accuracy: 0.9539\n",
            "Epoch 59/60\n",
            "200/200 [==============================] - 27s 134ms/step - loss: 0.1285 - accuracy: 0.9530\n",
            "Epoch 60/60\n",
            "200/200 [==============================] - 27s 135ms/step - loss: 0.1259 - accuracy: 0.9545\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XH8QYuOYc4_W"
      },
      "source": [
        "## Save latent space\n",
        "Once the learning is done, we can save the position of the songs in the latent space with the following code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "61k98_URc4_W",
        "outputId": "0d508553-d11b-4d09-8708-6ddce9389722"
      },
      "source": [
        "# retrieve the tracks positions in the projection space\n",
        "vectors_tracks = Track2Vec.get_weights()[0]\n",
        "with open(\"result/latent_positions.npy\", \"wb\") as f:\n",
        "    np.save(f, vectors_tracks)\n",
        "\n",
        "# info visualization\n",
        "print(\"first track position (30 dimensions):\\n\" + str(vectors_tracks[0]))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "first track position (30 dimensions):\n",
            "[ 0.02344619  0.03580823  0.02686919 -0.01389409  0.02441807  0.01420432\n",
            " -0.01518399 -0.0377614   0.01022662 -0.04573558 -0.00276524  0.02158012\n",
            " -0.00873798 -0.00718191 -0.00675355  0.02202994  0.01106698 -0.0160887\n",
            " -0.00503298 -0.02465426 -0.01198023  0.03262806  0.00116764 -0.01458681\n",
            "  0.00299946 -0.02654702 -0.02195956  0.05242296 -0.01643946 -0.01570888]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UvGlm02Yc4_Z"
      },
      "source": [
        "And latter load it with :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "OonqrCdXc4_a"
      },
      "source": [
        "vectors_tracks = np.load(\"result/latent_positions.npy\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0s8PoNQmc4_b"
      },
      "source": [
        "## Use in completion and evaluation\n",
        "We can now use this space to make suggestions. <span style=\"color:red\">Build a predict_batch function that takes as input a number vector of songs (seeds), (s) a number of suggestions to make per request, the vectors of the songs in the latent space X and a kd-tree to speed up the computation of closest neighbors. To make its propositions this function will return the indices of the s closest neighbors of each seed. </span> So that these predictions don't take too much time you will use a kd-tree (available in scikit learn) to speed up the search for nearest neighbors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "-974KwxZc4_b"
      },
      "source": [
        "from sklearn.neighbors import KDTree\n",
        "kdt = KDTree(vectors_tracks, leaf_size=10, metric='euclidean')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "3NkEGSGdc4_b"
      },
      "source": [
        "# The predict_batch function returns the indexes of the k-nearest neighbors of a vector of songs (seeds)\n",
        "# using a KD-tree to speed up the process. In particular we use the query function.\n",
        "def predict_batch(seeds, k, X, kdt):\n",
        "    return kdt.query(X=X[seeds], k=k, return_distance=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ueT6XqS8c4_b"
      },
      "source": [
        "<span style=\"color:red\">Use this function to propose songs to complete the playlist of the validation dataset (the seeds correspond to the first column of play_val).</span>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "-ZdAyoHWc4_c",
        "outputId": "9c4c7946-769c-4600-98cb-8916a26d70ed"
      },
      "source": [
        "indexes = predict_batch(play_val[:,0], 10, vectors_tracks, kdt)\n",
        "\n",
        "#info visualization\n",
        "print(\"10 closest points for \" + str(play_val[0, 0]) + \": \" + str(indexes[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10 closest points for 79029: [ 79029  94595  59293  94617  28266  47002 106017  94594  69891  89717]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_w20e7ec4_c"
      },
      "source": [
        "<span style=\"color:red\">Compare these suggestions with the second column of play_val (the songs actually present). To do this you will calculate the hit@10 which is 1 if the song actually present in the playlist is one of the 10 suggestions (this score is averaged over the validation set) and the NDCG@10 (Normalized Discounted Cumulative Gain) which takes into account the order of the suggestions. This second score is worth $1/log2(k+1)$ if proposal k (k between 1 and 10) is the correct proposal and 0 if no proposal is correct. As before you will calculate the average score on the validation set. </span>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "gTAV8L-dc4_c",
        "outputId": "be286bc2-1061-485e-ce60-783b8fd43d34"
      },
      "source": [
        "import math\n",
        "n = len(play_val)\n",
        "# This function computes the NDCGatK score, it indicates how correct the predictions are, giving importance to  # the location of the prediction\n",
        "def compute_NDCGatK(indexes):\n",
        "    NDCGatK = 0\n",
        "    for i in range(n):\n",
        "        if play_val[i, 1] in indexes[i]:\n",
        "            # We take the index of the prediction\n",
        "            index = list(indexes[i]).index(play_val[i, 1]) + 1\n",
        "\n",
        "            #We use this index to calculate the score\n",
        "            NDCGatK += 1 / math.log(index + 1, 2)\n",
        "    # Average        \n",
        "    NDCGatK /= n\n",
        "    return NDCGatK\n",
        "\n",
        "        \n",
        "print(\"NDCG@10: \" + str(compute_NDCGatK(indexes)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NDCG@10: 0.09613142578539993\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "l8e28Bbdc4_d",
        "outputId": "c62940f7-d1bb-421a-99bb-7cc4534da84b"
      },
      "source": [
        "# This function computes the HitatK score, it indicates how correct the predictions are, without giving \n",
        "# importance to the location of the prediction\n",
        "def compute_HitatK(indexes):\n",
        "    # We count all the times that the song in the second column of play_val is in our predictions list,\n",
        "    # then we calculate the average\n",
        "    HitatK = sum([1 for i in range(n) if play_val[i, 1] in indexes[i]]) / n\n",
        "    return HitatK\n",
        "\n",
        "#info visualization\n",
        "print(\"Hit@10: \" + str(compute_HitatK(indexes)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hit@10: 0.1583\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-51ioAYc4_d"
      },
      "source": [
        "## hyper parameters tunning\n",
        "\n",
        "<span style=\"color:red\">You can now try to vary the hyper parameters to improve your performance. Pay attention to the computing time : prepare a grid with about ten different configurations and evaluate each of them on your validation set.\n",
        "Evaluate the final performance of the best configuration found on the test set. Don't forget to save your results.</span>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GEo02bHBc4_d"
      },
      "source": [
        "###### We decided to use the NDCGatK score as the primary metric to decide which is the best model ######\n",
        "\n",
        "# The dictionary with all the results\n",
        "results = {}\n",
        "\n",
        "# The list with the NDCGatK scores\n",
        "NDCGatK_results = []\n",
        "\n",
        "# Define the hyper parameters\n",
        "hyper_parameters = {\n",
        "    \"epochs\": range(40, 81, 20),\n",
        "    \"steps_per_epoch\": range(200, 301, 100),\n",
        "    \"min_batch_size\": range(50, 101, 50)\n",
        "}\n",
        "\n",
        "# Try all the permutations with the hyper parameters above\n",
        "i = 0\n",
        "for epochs in hyper_parameters[\"epochs\"]:\n",
        "    for steps in hyper_parameters[\"steps_per_epoch\"]:\n",
        "        for batch_size in hyper_parameters[\"min_batch_size\"]:\n",
        "            hist = Track2Vec.fit(track_ns_generator(play_app, batch_size), steps_per_epoch=steps, epochs=epochs, verbose=0)\n",
        "            vectors_tracks = Track2Vec.get_weights()[0]\n",
        "            indexes = predict_batch(play_val[:,0], 10, vectors_tracks, kdt)\n",
        "            NDCGatK = compute_NDCGatK(indexes)\n",
        "            NDCGatK_results.append(NDCGatK)\n",
        "            results[i] = {\n",
        "                \"hyper_parameters\": {\n",
        "                    \"min_batch_size\": batch_size,\n",
        "                    \"steps_per_epoch\": steps,\n",
        "                    \"epochs\": epochs,\n",
        "                },\n",
        "                \"metrics\": {\n",
        "                    \"accuracy\": hist.history[\"accuracy\"][-1],\n",
        "                    \"loss\": hist.history[\"loss\"][-1],\n",
        "                    \"NDGC@10\": NDCGatK,\n",
        "                    \"Hit@10\": compute_HitatK(indexes),\n",
        "                },\n",
        "                \"vectors_tracks\": vectors_tracks,\n",
        "            }\n",
        "            i += 1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# The index of the best configuration, the one with maximum NDCGatK\n",
        "best_index = NDCGatK_results.index(max(NDCGatK_results))\n",
        "\n",
        "# Save the results\n",
        "with open('result/best_latent_positions.npy', 'wb') as f:\n",
        "    np.save(f, results[best_index][\"vectors_tracks\"])\n",
        "\n",
        "# Load the results\n",
        "vectors_tracks = np.load(\"result/best_latent_positions.npy\")\n",
        "\n",
        "# Create the KDTree\n",
        "kdt = KDTree(vectors_tracks, leaf_size=10, metric='euclidean')\n",
        "\n",
        "# info visualization\n",
        "print(\"Index of the best configuration: \" + str(best_index))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Gw5Iwaxc4_e"
      },
      "source": [
        "import pandas as pd\n",
        "import csv\n",
        "\n",
        "# Write the results in a csv file\n",
        "with open('result/results.csv', 'w') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow([\"id\", \"min_batch_size\", \"steps_per_epoch\", \"epochs\", \"accuracy\", \"loss\", \"NDGC@10\", \"Hit@10\"])\n",
        "    for id in results:\n",
        "        writer.writerow([id] + [results[id][super_key][key] \n",
        "                        for super_key in results[id] if super_key != \"vectors_tracks\"\n",
        "                            for key in results[id][super_key]])\n",
        "\n",
        "results_df = pd.read_csv(\"result/results.csv\")\n",
        "results_df.set_index(\"id\", inplace=True)\n",
        "results_df\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82mUDSV2c4_f"
      },
      "source": [
        "## Bonus, a little music\n",
        "\n",
        "The TrackArtists file contains meta.data on the tracks and the artists for a subset of the 300,000 tracks in the dataset. We can use it to search for the number of a song from its title:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pind6OXvc4_g"
      },
      "source": [
        "tr_meta = pd.read_csv(\"data/TracksArtists.csv\")\n",
        "joindf = pd.DataFrame({\"track_id\": tracks_list_ordered[:Vt], \"index\": range(Vt)})\n",
        "meta = tr_meta.merge(joindf, left_on=\"track_id\", right_on=\"track_id\")\n",
        "meta.set_index(\"index\", inplace=True)\n",
        "meta[[\"title\", \"name\", \"preview\", \"track_id\"]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hIDmVMWxc4_g"
      },
      "source": [
        "def find_track(title):\n",
        "    return meta.loc[meta[\"title\"]==title,:].index[0]\n",
        "\n",
        "tr = find_track(\"Hexagone\")\n",
        "\n",
        "# info visualization\n",
        "print(\"index of 'Hexagone' track: \" + str(tr))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWdppt6hc4_g"
      },
      "source": [
        "## Radio\n",
        "\n",
        "The deeezer api allows you to retrieve information about the pieces of the dataset from their deezer id. Among this information when it is available a url to listen to a free sample is provided."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MV38xpySc4_h"
      },
      "source": [
        "import urllib.request, json \n",
        "def gettrackinfo(number):\n",
        "    track_url =  \"https://api.deezer.com/track/{}\".format(tracks_list_ordered[number].split(\"_\")[1])\n",
        "    with urllib.request.urlopen(track_url) as url:\n",
        "        data = json.loads(url.read().decode())\n",
        "    return data\n",
        "track_apidata = gettrackinfo(find_track(\"Hexagone\"))\n",
        "\n",
        "# info visualization\n",
        "print(\"info about 'Hexagone' track:\")\n",
        "for key in track_apidata:\n",
        "    print(\"\\t\" + str(key) + \": \" + str(track_apidata[key]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2-9xlHsc4_h"
      },
      "source": [
        "So we can use it to listen a preview:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4PGznKAc4_h"
      },
      "source": [
        "from IPython.display import display, Audio, clear_output\n",
        "display(Audio(track_apidata[\"preview\"], autoplay=True))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HSkdsYsYc4_i"
      },
      "source": [
        "<span style=\"color:red\">Create a radio function that takes as input a track number in the dataset and launches a series of nb_track tracks by randomly pulling in the neighborhood of the current track the next track to listen to. The size of the neighborhood will be configurable and you will delete from the proposals the songs already listened to. You will handle exceptions if the track does not have an available extract. You can delete the current song with the clear_display function.</span>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evS-q88kc4_i"
      },
      "source": [
        "import time\n",
        "def start_radio(seed, nb_candidates, duration, nbsteps=20):\n",
        "    print(meta.loc[seed, \"title\"])\n",
        "    display(Audio(meta.loc[seed, \"preview\"], autoplay=True))\n",
        "    time.sleep(duration)\n",
        "    clear_output()\n",
        "    already_played = [seed]\n",
        "    listened = [meta.loc[seed, \"title\"]]\n",
        "    while nbsteps > 0:\n",
        "        try:\n",
        "            # Recommended nb_candidates tracks given the seed using the predict_batch function\n",
        "            recommended_tracks = predict_batch([seed], nb_candidates, vectors_tracks, kdt)[0]\n",
        "            # If the current seed was already played we choose another seed from the recommended tracks\n",
        "            while seed in already_played:\n",
        "                seed = recommended_tracks[random.randint(1, nb_candidates - 1)]\n",
        "            # Add the new seed in the list of the already played songs\n",
        "            already_played.append(seed)\n",
        "            print(meta.loc[seed, \"title\"])\n",
        "            display(Audio(meta.loc[seed, \"preview\"], autoplay=True))\n",
        "            time.sleep(duration)\n",
        "            # Add the title of the song listened in the \"listened\" list\n",
        "            listened.append(meta.loc[seed, \"title\"])\n",
        "            nbsteps -= 1\n",
        "        except:\n",
        "            print(\"track with id \" + str(seed) + \" not found in meta dataframe\")\n",
        "            pass\n",
        "        clear_output()\n",
        "    print(\"Thanks for listening our radio!\\nYou have listened the following songs:\\n\\t\" + \"\\n\\t\".join(listened))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OsUCXxCWc4_i"
      },
      "source": [
        "start_radio(find_track(\"Hexagone\"), 5, 5, 10)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}