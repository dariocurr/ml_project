{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deezer playlist dataset and song recommendation with word2vec\n",
    "\n",
    "In this mini project we will develop a word2vec network and use it to build a playlist completion tool (song suggestion). The data is hosted on the following repository: http://github.com/comeetie/deezerplay.git. To know more about word2vec and these data you can read the two following references:\n",
    "\n",
    "- Efficient estimation of word representations in vector space, Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. (https://arxiv.org/abs/1301.3781)\n",
    "- Word2with applied to Recommendation: Hyperparameters Matter, H. Caselles-DuprÃ©, F. Lesaint and J. Royo-Letelier. (https://arxiv.org/pdf/1804.04212.pdf)\n",
    "\n",
    "The elements you have to do are highlighted in red."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation of data\n",
    "\n",
    "The data is in the form of a playlist list. Each playlist is a list with the deezer ID of the psong followed by the artist ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[100000, 24.21338]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "data = np.load(\"data/music_2.npy\", allow_pickle=True)\n",
    "[len(data), np.mean([len(p) for p in data])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset we are going to work on contains 100000 playlists which are composed of an average of 24.1 songs. We will start by keeping only the song identifiers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "playlist_track = [list(filter(lambda w: w.split(\"_\")[0] == u\"track\", playlist)) for playlist in data]\n",
    "playlist_artist = [list(filter(lambda w: w.split(\"_\")[0] == u\"artist\", playlist)) for playlist in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "338509"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# songs != playlists\n",
    "tracks = np.unique(np.concatenate(playlist_track))\n",
    "Vt = len(tracks)\n",
    "Vt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of different songs in this data-set is quite high with more than 300,000 songs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a song dictionary\n",
    "We will assign to each song an integer that will serve as a unique identifier and input for our network. In order to save a little bit of resources we will only work in this project on songs that appear in at least two playlists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# counting occurences for each track\n",
    "track_counts = dict((tracks[i], 0) for i in range(0, Vt))\n",
    "for p in playlist_track:\n",
    "    for track in p:\n",
    "        track_counts[track] = track_counts[track] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter very rare songs to save ressources\n",
    "playlist_track_filter = [list(filter(lambda track : track_counts[track] > 1, playlist)) for playlist in playlist_track]\n",
    "# get the counts\n",
    "counts  =  np.array(list(track_counts.values()))\n",
    "# sort\n",
    "order = np.argsort(-counts)\n",
    "# deezed_id array\n",
    "tracks_list_ordered = np.array(list(track_counts.keys()))[order]\n",
    "# Vocabulary size = number of kept songs\n",
    "Vt = np.where(counts[order] == 1)[0][0]\n",
    "# dict construction id_morceaux num_id [0,Vt]\n",
    "track_dict = dict((tracks_list_ordered[i], i) for i in range(0, Vt))\n",
    "# playlist conversion to list of integers\n",
    "corpus_num_track = [[track_dict[track] for track in play ] for play in playlist_track_filter]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation of test and validation learning sets\n",
    "\n",
    "To learn the parameters of our method we will keep the first l-1 songs of each playlist (with l the length of the playlist) for learning. To evaluate the completion performance of our method we keep for each playlist the last two songs. The objective will be to find the last one from the next-to-last one. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93892\n",
      "[23294  2016 79201 ...   173 33678   924]\n"
     ]
    }
   ],
   "source": [
    "# playlist main part used for trainning\n",
    "play_app  = [corpus_num_track[i][:(len(corpus_num_track[i])-1)] for i in range(len(corpus_num_track)) if len(corpus_num_track[i]) > 1]\n",
    "\n",
    "# the two last elements are used for validation and training\n",
    "index_tst = np.random.choice(100000,20000)\n",
    "index_val = np.setdiff1d(range(100000),index_tst)\n",
    "\n",
    "play_tst  = np.array([corpus_num_track[i][(len(corpus_num_track[i])-2):len(corpus_num_track[i])] \n",
    "             for i in index_tst if len(corpus_num_track[i])>3])\n",
    "play_val  = np.array([corpus_num_track[i][(len(corpus_num_track[i])-2):len(corpus_num_track[i])] \n",
    "             for i in index_val if len(corpus_num_track[i])>3])[:10000]\n",
    "\n",
    "print(play_val[:,1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Embedding, Reshape, Activation, Input, Dense, Flatten\n",
    "from keras.layers.merge import Dot\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.sequence import skipgrams, make_sampling_table\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hyper-parameters of word2vec :\n",
    "\n",
    "the method word2vec needs some hyper-parameters. We are going to give them the first values, but we will refine them later:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# latent space dimension\n",
    "# the size of each of our word embedding vectors\n",
    "vector_dim = 30\n",
    "# window size\n",
    "# the window of words around the target word that will be used to draw the context words from\n",
    "window_width = 3\n",
    "# number of negative sample per positive sample\n",
    "neg_sample = 5\n",
    "# taille des mini-batch\n",
    "min_batch_size = 50\n",
    "# smoothing factor for the sampling table of negative pairs \n",
    "samp_coef = 0.5\n",
    "# cparameter to sub-sample frequent song\n",
    "sub_samp = 0.00001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation of the draw probability tables (smoothed) and unsmoothed\n",
    "\n",
    "To draw the negative examples we need the smoothed frequencies of each song in our dataset. Likewise to under-sample very frequent pieces we need the raw frequencies. We will calculate these two vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the counts\n",
    "counts = np.array(list(track_counts.values()),dtype='float')[order[:Vt]]\n",
    "# normalization\n",
    "st =  counts/np.sum(counts)\n",
    "# smoothing\n",
    "st_smooth = np.power(st,samp_coef)\n",
    "st_smooth = st_smooth/np.sum(st_smooth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the word2 network with\n",
    "\n",
    "A word2 network with takes in input two integers corresponding to two songs, these are embedded in a latent space of dimension (vector_dim) thanks to a layer of embedding type (you will have to use the same layer to project the two pieces). Once these two vectors have been extracted, the array must calculate their scalar product normalize appleler cosine distance : \n",
    "\n",
    "$$cos(\\theta_{ij})=\\frac{z_i.z_j}{||z_i||||z_j||}$$\n",
    "\n",
    "To carry out this treatment you will use a \"dot\" layer for \"dot product\". The model then uses a sigmoid layer to produce the output. This output will be 0 when both songs are randomly drawn from the whole dataset and 1 when they were extracted from the same playslist. <span style=\"color:red\">You have to create the keras Track2Vec model corresponding to this architecture.</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs\n",
    "\n",
    "input_target = Input((1,), dtype='int32')\n",
    "input_context = Input((1,), dtype='int32')\n",
    "\n",
    "embedding = Embedding(Vt, vector_dim, input_length=1, name='embedding')\n",
    "target = embedding(input_target)\n",
    "context = embedding(input_context)\n",
    "dot_product = Dot(axes=2)([target, context])\n",
    "flatten = Flatten()(dot_product)\n",
    "output = Dense(1, activation='sigmoid',name=\"classif\")(flatten)\n",
    "\n",
    "Track2Vec = Model(inputs=[input_target, input_context], outputs=output)\n",
    "Track2Vec.compile(loss='binary_crossentropy', optimizer='adam', metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 1, 30)        3697230     input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dot (Dot)                       (None, 1, 1)         0           embedding[0][0]                  \n",
      "                                                                 embedding[1][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 1)            0           dot[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "classif (Dense)                 (None, 1)            2           flatten[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 3,697,232\n",
      "Trainable params: 3,697,232\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "Track2Vec.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation of the data generator\n",
    "\n",
    "To learn the projection layer at the heart of our model we will build a generator of positive and negative pair examples of close or random songs from our training data. The following function will allow us to generate such examples from a playlist (seq) provided as input. This function will first build all the pairs of songs that can be extracted from the playlist if they are within (windows) distance of each other. These pairs will constitute the positive pairs. The pairs concerning very frequent songs will be removed with a probability that depends on their frequencies. Finally a number of negative examples (corresponding to neg_samples * positive number of examples) will be randomly drawn using the neg_sampling_table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to generate word2vec positive and negative pairs \n",
    "# from an array of int that represent a text ot here a playlist\n",
    "# params \n",
    "# seq : input text or playlist (array of int)\n",
    "# neg_samples : number of negative sample to generate per positive ones\n",
    "# neg_sampling_table : sampling table for negative samples\n",
    "# sub sampling_table : sampling table for sub sampling common words songs\n",
    "# sub_t : sub sampling parameter\n",
    "def word2vecSampling(seq, window, neg_samples, neg_sampling_table, sub_sampling_table, sub_t):\n",
    "    # vocab size\n",
    "    V = len(neg_sampling_table)\n",
    "    # extract positive pairs \n",
    "    positives = skipgrams(sequence=seq, vocabulary_size=V, window_size=window, negative_samples=0) #return couples, labels: where couples are int pairs and labels are either 0 or 1.\n",
    "    ppairs    = np.array(positives[0]) #couples\n",
    "    # sub sampling\n",
    "    if (ppairs.shape[0]>0):\n",
    "        f = sub_sampling_table[ppairs[:,0]]\n",
    "        subprob = ((f-sub_t)/f)-np.sqrt(sub_t/f)\n",
    "        tokeep = (subprob<np.random.uniform(size=subprob.shape[0])) | (subprob<0)\n",
    "        ppairs = ppairs[tokeep,:]\n",
    "    nbneg     = ppairs.shape[0]*neg_samples\n",
    "    # sample negative pairs\n",
    "    if (nbneg > 0):\n",
    "        negex     = np.random.choice(V, nbneg, p=neg_sampling_table)\n",
    "        negexcontext = np.repeat(ppairs[:,0],neg_samples)\n",
    "        npairs    = np.transpose(np.stack([negexcontext,negex]))\n",
    "        pairs     = np.concatenate([ppairs,npairs],axis=0)\n",
    "        labels    = np.concatenate([np.repeat(1,ppairs.shape[0]),np.repeat(0,nbneg)])\n",
    "        perm      = np.random.permutation(len(labels))\n",
    "        res = [pairs[perm,:],labels[perm]]\n",
    "    else:\n",
    "        res=[[],[]]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">Use this function to build a \"track_ns_generator\" of data which will generate positive and negative examples from \"nbm\" playlists randomly drawn from the \"corpus_num\" dataset provided as input. </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def track_ns_generator(corpus_num,nbm):\n",
    "    \n",
    "    \n",
    "    while 1: \n",
    "        x = np.ndarray((0,2), dtype=np.int32)\n",
    "        y = np.ndarray((0),dtype=np.int32)\n",
    "        \n",
    "        for i in range(nbm):\n",
    "            randint = random.randint(0, len(corpus_num))\n",
    "            a,b = word2vecSampling(corpus_num[randint], window_width, neg_sample, st_smooth, st, sub_samp)\n",
    "\n",
    "            if(len(a) > 0):\n",
    "                x = np.vstack((x,a))\n",
    "                y = np.append(y,b)\n",
    "                \n",
    "        yield ((x[:,0],x[:,1]), y)\n",
    "        #yield (x, y)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning \n",
    "You should now be able to learn your first model with the following code. This should take between 15 and 30 min."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "200/200 [==============================] - 26s 127ms/step - loss: 0.6771 - accuracy: 0.8140\n",
      "Epoch 2/60\n",
      "200/200 [==============================] - 33s 164ms/step - loss: 0.6203 - accuracy: 0.8333\n",
      "Epoch 3/60\n",
      "200/200 [==============================] - 33s 163ms/step - loss: 0.5744 - accuracy: 0.8334\n",
      "Epoch 4/60\n",
      "200/200 [==============================] - 33s 164ms/step - loss: 0.5337 - accuracy: 0.8344\n",
      "Epoch 5/60\n",
      "200/200 [==============================] - 32s 163ms/step - loss: 0.4911 - accuracy: 0.8403\n",
      "Epoch 6/60\n",
      "200/200 [==============================] - 33s 164ms/step - loss: 0.4414 - accuracy: 0.8590\n",
      "Epoch 7/60\n",
      "200/200 [==============================] - 33s 164ms/step - loss: 0.3884 - accuracy: 0.8842\n",
      "Epoch 8/60\n",
      "200/200 [==============================] - 32s 159ms/step - loss: 0.3386 - accuracy: 0.9262\n",
      "Epoch 9/60\n",
      "200/200 [==============================] - 32s 162ms/step - loss: 0.2911 - accuracy: 0.9637\n",
      "Epoch 10/60\n",
      "200/200 [==============================] - 32s 161ms/step - loss: 0.2518 - accuracy: 0.9778\n",
      "Epoch 11/60\n",
      "200/200 [==============================] - 33s 163ms/step - loss: 0.2200 - accuracy: 0.9824\n",
      "Epoch 12/60\n",
      "200/200 [==============================] - 32s 161ms/step - loss: 0.1943 - accuracy: 0.9842\n",
      "Epoch 13/60\n",
      "200/200 [==============================] - 32s 162ms/step - loss: 0.1728 - accuracy: 0.9854\n",
      "Epoch 14/60\n",
      "200/200 [==============================] - 32s 162ms/step - loss: 0.1551 - accuracy: 0.9865\n",
      "Epoch 15/60\n",
      "200/200 [==============================] - 32s 162ms/step - loss: 0.1408 - accuracy: 0.9869s - loss: 0.1408 - accuracy: 0.98\n",
      "Epoch 16/60\n",
      "200/200 [==============================] - 32s 159ms/step - loss: 0.1276 - accuracy: 0.9877\n",
      "Epoch 17/60\n",
      "200/200 [==============================] - 32s 159ms/step - loss: 0.1166 - accuracy: 0.9885\n",
      "Epoch 18/60\n",
      "200/200 [==============================] - 33s 165ms/step - loss: 0.1075 - accuracy: 0.9888\n",
      "Epoch 19/60\n",
      "200/200 [==============================] - 32s 161ms/step - loss: 0.0993 - accuracy: 0.9895\n",
      "Epoch 20/60\n",
      "200/200 [==============================] - 32s 161ms/step - loss: 0.0919 - accuracy: 0.9898\n",
      "Epoch 21/60\n",
      "200/200 [==============================] - 32s 160ms/step - loss: 0.0852 - accuracy: 0.9906\n",
      "Epoch 22/60\n",
      "200/200 [==============================] - 32s 161ms/step - loss: 0.0790 - accuracy: 0.9911\n",
      "Epoch 23/60\n",
      "200/200 [==============================] - 32s 160ms/step - loss: 0.0742 - accuracy: 0.9911\n",
      "Epoch 24/60\n",
      "200/200 [==============================] - 32s 161ms/step - loss: 0.0691 - accuracy: 0.9916\n",
      "Epoch 25/60\n",
      "200/200 [==============================] - 32s 161ms/step - loss: 0.0648 - accuracy: 0.9921\n",
      "Epoch 26/60\n",
      "200/200 [==============================] - 32s 161ms/step - loss: 0.0609 - accuracy: 0.9924\n",
      "Epoch 27/60\n",
      "200/200 [==============================] - 32s 158ms/step - loss: 0.0578 - accuracy: 0.9923\n",
      "Epoch 28/60\n",
      "200/200 [==============================] - 32s 161ms/step - loss: 0.0539 - accuracy: 0.9929\n",
      "Epoch 29/60\n",
      "200/200 [==============================] - 32s 162ms/step - loss: 0.0513 - accuracy: 0.9931\n",
      "Epoch 30/60\n",
      "200/200 [==============================] - 32s 162ms/step - loss: 0.0487 - accuracy: 0.9932\n",
      "Epoch 31/60\n",
      "200/200 [==============================] - 32s 160ms/step - loss: 0.0460 - accuracy: 0.9934\n",
      "Epoch 32/60\n",
      "200/200 [==============================] - 32s 159ms/step - loss: 0.0434 - accuracy: 0.9939\n",
      "Epoch 33/60\n",
      "200/200 [==============================] - 32s 162ms/step - loss: 0.0413 - accuracy: 0.9941\n",
      "Epoch 34/60\n",
      "200/200 [==============================] - 32s 161ms/step - loss: 0.0397 - accuracy: 0.9939\n",
      "Epoch 35/60\n",
      "200/200 [==============================] - 32s 160ms/step - loss: 0.0373 - accuracy: 0.9943\n",
      "Epoch 36/60\n",
      "200/200 [==============================] - 32s 160ms/step - loss: 0.0358 - accuracy: 0.9945\n",
      "Epoch 37/60\n",
      "200/200 [==============================] - 32s 158ms/step - loss: 0.0341 - accuracy: 0.9947\n",
      "Epoch 38/60\n",
      "200/200 [==============================] - 32s 161ms/step - loss: 0.0328 - accuracy: 0.9947s - loss: 0.0328 - accuracy: 0.99\n",
      "Epoch 39/60\n",
      "200/200 [==============================] - 32s 161ms/step - loss: 0.0308 - accuracy: 0.9951\n",
      "Epoch 40/60\n",
      "200/200 [==============================] - 32s 161ms/step - loss: 0.0300 - accuracy: 0.9951\n",
      "Epoch 41/60\n",
      "200/200 [==============================] - 32s 161ms/step - loss: 0.0289 - accuracy: 0.9951\n",
      "Epoch 42/60\n",
      "200/200 [==============================] - 32s 160ms/step - loss: 0.0273 - accuracy: 0.9953\n",
      "Epoch 43/60\n",
      "200/200 [==============================] - 32s 161ms/step - loss: 0.0263 - accuracy: 0.9955\n",
      "Epoch 44/60\n",
      "200/200 [==============================] - 32s 162ms/step - loss: 0.0253 - accuracy: 0.9956\n",
      "Epoch 45/60\n",
      "200/200 [==============================] - 32s 161ms/step - loss: 0.0249 - accuracy: 0.9955\n",
      "Epoch 46/60\n",
      "200/200 [==============================] - 32s 161ms/step - loss: 0.0236 - accuracy: 0.9957\n",
      "Epoch 47/60\n",
      "200/200 [==============================] - 32s 160ms/step - loss: 0.0231 - accuracy: 0.9958\n",
      "Epoch 48/60\n",
      "200/200 [==============================] - 32s 161ms/step - loss: 0.0219 - accuracy: 0.9960\n",
      "Epoch 49/60\n",
      "200/200 [==============================] - 32s 160ms/step - loss: 0.0217 - accuracy: 0.9960\n",
      "Epoch 50/60\n",
      "200/200 [==============================] - 32s 159ms/step - loss: 0.0207 - accuracy: 0.9961\n",
      "Epoch 51/60\n",
      "200/200 [==============================] - 32s 160ms/step - loss: 0.0201 - accuracy: 0.9961s - loss: 0.0201 - accuracy: 0.99\n",
      "Epoch 52/60\n",
      "200/200 [==============================] - 32s 160ms/step - loss: 0.0194 - accuracy: 0.9963\n",
      "Epoch 53/60\n",
      "200/200 [==============================] - 32s 159ms/step - loss: 0.0187 - accuracy: 0.9964\n",
      "Epoch 54/60\n",
      "200/200 [==============================] - 32s 158ms/step - loss: 0.0180 - accuracy: 0.9964\n",
      "Epoch 55/60\n",
      "200/200 [==============================] - 32s 162ms/step - loss: 0.0174 - accuracy: 0.9966\n",
      "Epoch 56/60\n",
      "200/200 [==============================] - 33s 163ms/step - loss: 0.0173 - accuracy: 0.9965\n",
      "Epoch 57/60\n",
      "200/200 [==============================] - 32s 160ms/step - loss: 0.0167 - accuracy: 0.9966\n",
      "Epoch 58/60\n",
      "200/200 [==============================] - 28s 137ms/step - loss: 0.0162 - accuracy: 0.9967\n",
      "Epoch 59/60\n",
      "200/200 [==============================] - 16s 81ms/step - loss: 0.0157 - accuracy: 0.9968\n",
      "Epoch 60/60\n",
      "200/200 [==============================] - 16s 82ms/step - loss: 0.0155 - accuracy: 0.9968\n"
     ]
    }
   ],
   "source": [
    "hist=Track2Vec.fit(track_ns_generator(play_app,min_batch_size),steps_per_epoch = 200,epochs=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save latent space\n",
    "Once the learning is done, we can save the position of the songs in the latent space with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rÃ©cupÃ©rations des positions des morceaux dans l'espace de projection\n",
    "vectors_tracks = Track2Vec.get_weights()[0]\n",
    "\n",
    "with open('latent_positions.npy', 'wb') as f:\n",
    "    np.save(f, vectors_tracks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And latter load it with :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123241\n"
     ]
    }
   ],
   "source": [
    "vectors_tracks=np.load(\"latent_positions.npy\")\n",
    "print(len(vectors_tracks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use in completion and evaluation\n",
    "We can now use this space to make suggestions. <span style=\"color:red\">Build a predict_batch function that takes as input a number vector of songs (seeds), (s) a number of suggestions to make per request, the vectors of the songs in the latent space X and a kd-tree to speed up the computation of closest neighbors. To make its propositions this function will return the indices of the s closest neighbors of each seed. </span> So that these predictions don't take too much time you will use a kd-tree (available in scikit learn) to speed up the search for nearest neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123241\n",
      "[79029   532 50537 ...  4161  8281  1696]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KDTree\n",
    "kdt = KDTree(vectors_tracks, leaf_size=10, metric='euclidean')\n",
    "print(len(kdt.data))\n",
    "print(play_val[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_batch(seeds, k, X, kdt):\n",
    "    ind = kdt.query(X, k, return_distance = False)\n",
    "    return ind[seeds]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">Use this function to propose songs to complete the playlist of the validation dataset (the seeds correspond to the first column of play_val).</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 79029  30045  87944 ...  14138  63323  47837]\n",
      " [   532   2995  78182 ...   1039  28655  26763]\n",
      " [ 50537  75167  35510 ...  13606  23544  77281]\n",
      " ...\n",
      " [ 18195  12075  43924 ...   7709  43634  33916]\n",
      " [  9609  17388   2344 ... 118516  88184  93265]\n",
      " [     8  98125    206 ...  35305  53617    893]]\n"
     ]
    }
   ],
   "source": [
    "indexes = predict_batch(play_val[:,0],10,vectors_tracks,kdt)\n",
    "print(indexes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">Compare these suggestions with the second column of play_val (the songs actually present). To do this you will calculate the hit@10 which is 1 if the song actually present in the playlist is one of the 10 suggestions (this score is averaged over the validation set) and the NDCG@10 (Normalized Discounted Cumulative Gain) which takes into account the order of the suggestions. This second score is worth $1/log2(k+1)$ if proposal k (k between 1 and 10) is the correct proposal and 0 if no proposal is correct. As before you will calculate the average score on the validation set. </span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'indexes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-e286c551fee8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mNDGCatK\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mplay_val\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mplay_val\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mindexes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m             \u001b[0mNDGCatK\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'indexes' is not defined"
     ]
    }
   ],
   "source": [
    "import math\n",
    "def NDGCatK(indexes):\n",
    "    NDGCatK = 0\n",
    "    for i in range(len(play_val[:,1])):\n",
    "        for j in range(len(indexes[i])):\n",
    "            if play_val[i,1] == indexes[i][j]:\n",
    "                NDGCatK += 1/(math.log(j+1,2))\n",
    "\n",
    "    NDGCatK = NDGCatK/len(play_val[:,1])\n",
    "    print(NDGCatK)\n",
    "    \n",
    "    \n",
    "NDGCatK(indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0925"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def HitatK(indexes):\n",
    "    HitatK = 0\n",
    "    for i in range(len(indexes)):\n",
    "        if play_val[i,1] in indexes[i]:\n",
    "            HitatK += 1\n",
    "    HitatK = HitatK/len(play_val[:,1])\n",
    "    print(HitatK)\n",
    "    \n",
    "HitatK(indexes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hyper parameters tunning\n",
    "\n",
    "<span style=\"color:red\">You can now try to vary the hyper parameters to improve your performance. Pay attention to the computing time : prepare a grid with about ten different configurations and evaluate each of them on your validation set.\n",
    "Evaluate the final performance of the best configuration found on the test set. Don't forget to save your results.</span>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possiamo cambiare: epochs (32,...), steps_per_epoch, batch_size, optimizer, forse anche la loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Singleton array array(<generator object track_ns_generator at 0x000001EF43C61F48>,\n      dtype=object) cannot be considered a valid collection.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-c7eb20000ac0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mparam_grid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mgrid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrack2Vec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoring\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mgrid_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgrid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    648\u001b[0m             \u001b[0mrefit_metric\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'score'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    649\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 650\u001b[1;33m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroups\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    651\u001b[0m         \u001b[0mfit_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_fit_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    652\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mindexable\u001b[1;34m(*iterables)\u001b[0m\n\u001b[0;32m    246\u001b[0m     \"\"\"\n\u001b[0;32m    247\u001b[0m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0m_make_indexable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[1;32min\u001b[0m \u001b[0miterables\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 248\u001b[1;33m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    249\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    250\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    206\u001b[0m     \"\"\"\n\u001b[0;32m    207\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m     \u001b[0mlengths\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0m_num_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mX\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m     \u001b[0muniques\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    206\u001b[0m     \"\"\"\n\u001b[0;32m    207\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m     \u001b[0mlengths\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0m_num_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mX\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m     \u001b[0muniques\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m_num_samples\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    150\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m             raise TypeError(\"Singleton array %r cannot be considered\"\n\u001b[1;32m--> 152\u001b[1;33m                             \" a valid collection.\" % x)\n\u001b[0m\u001b[0;32m    153\u001b[0m         \u001b[1;31m# Check that shape is returning an integer or default to len\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m         \u001b[1;31m# Dask dataframes may not return numeric shape[0] value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Singleton array array(<generator object track_ns_generator at 0x000001EF43C61F48>,\n      dtype=object) cannot be considered a valid collection."
     ]
    }
   ],
   "source": [
    "#1\n",
    "min_batch_size = 32\n",
    "hist1=Track2Vec.fit(track_ns_generator(play_app,min_batch_size),steps_per_epoch = 400,epochs=50)\n",
    "\n",
    "kdt = KDTree(vectors_tracks, leaf_size=10, metric='euclidean')\n",
    "indexes = predict_batch(play_val[:,0],10,vectors_tracks,kdt)\n",
    "NDGCatK(indexes)\n",
    "HitatK(indexes)\n",
    "\n",
    "\n",
    "\n",
    "#2\n",
    "min_batch_size = 64\n",
    "Track2Vec.compile(loss='binary_crossentropy', optimizer='SGD', metrics=[\"accuracy\"])\n",
    "hist2=Track2Vec.fit(track_ns_generator(play_app,min_batch_size),steps_per_epoch = 350,epochs=32)\n",
    "\n",
    "kdt = KDTree(vectors_tracks, leaf_size=10, metric='euclidean')\n",
    "indexes = predict_batch(play_val[:,0],10,vectors_tracks,kdt)\n",
    "NDGCatK(indexes)\n",
    "HitatK(indexes)\n",
    "\n",
    "\n",
    "\n",
    "#3\n",
    "min_batch_size = 64\n",
    "Track2Vec.compile(loss='binary_crossentropy', optimizer='adam', metrics=[\"accuracy\"])\n",
    "hist3=Track2Vec.fit(track_ns_generator(play_app,min_batch_size),steps_per_epoch = 350,epochs=32)\n",
    "\n",
    "kdt = KDTree(vectors_tracks, leaf_size=10, metric='euclidean')\n",
    "indexes = predict_batch(play_val[:,0],10,vectors_tracks,kdt)\n",
    "NDGCatK(indexes)\n",
    "HitatK(indexes)\n",
    "\n",
    "\n",
    "\n",
    "#4\n",
    "min_batch_size = 100\n",
    "Track2Vec.compile(loss='hinge', optimizer='adam', metrics=[\"accuracy\"])\n",
    "hist4=Track2Vec.fit(track_ns_generator(play_app,min_batch_size),steps_per_epoch = 200,epochs=50)\n",
    "\n",
    "kdt = KDTree(vectors_tracks, leaf_size=10, metric='euclidean')\n",
    "indexes = predict_batch(play_val[:,0],10,vectors_tracks,kdt)\n",
    "NDGCatK(indexes)\n",
    "HitatK(indexes)\n",
    "\n",
    "\n",
    "\n",
    "#5\n",
    "min_batch_size = 100\n",
    "Track2Vec.compile(loss='hinge', optimizer='adam', metrics=[\"accuracy\"])\n",
    "hist5=Track2Vec.fit(track_ns_generator(play_app,min_batch_size),steps_per_epoch = 200,epochs=50)\n",
    "\n",
    "kdt = KDTree(vectors_tracks, leaf_size=10, metric='euclidean')\n",
    "indexes = predict_batch(play_val[:,0],10,vectors_tracks,kdt)\n",
    "NDGCatK(indexes)\n",
    "HitatK(indexes)\n",
    "\n",
    "\n",
    "#6\n",
    "min_batch_size = 64\n",
    "Track2Vec.compile(loss='squared_hinge', optimizer='adam', metrics=[\"accuracy\"])\n",
    "hist5=Track2Vec.fit(track_ns_generator(play_app,min_batch_size),steps_per_epoch = 200,epochs=32)\n",
    "\n",
    "kdt = KDTree(vectors_tracks, leaf_size=10, metric='euclidean')\n",
    "indexes = predict_batch(play_val[:,0],10,vectors_tracks,kdt)\n",
    "NDGCatK(indexes)\n",
    "HitatK(indexes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus, a little music\n",
    "\n",
    "The TrackArtists file contains meta.data on the tracks and the artists for a subset of the 300,000 tracks in the dataset. We can use it to search for the number of a song from its title:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "tr_meta=pd.read_csv(\"./TracksArtists.csv\")\n",
    "joindf = pd.DataFrame({\"track_id\":tracks_list_ordered[:Vt],\"index\":range(Vt)})\n",
    "meta = tr_meta.merge(joindf, left_on=\"track_id\",right_on=\"track_id\")\n",
    "meta.set_index(\"index\",inplace=True)\n",
    "meta[[\"title\",\"name\",\"preview\",\"track_id\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_track(title):\n",
    "    return meta.loc[meta[\"title\"]==title,:].index[0]\n",
    "\n",
    "tr=find_track(\"Hexagone\")\n",
    "tr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Radio\n",
    "\n",
    "The deeezer api allows you to retrieve information about the pieces of the dataset from their deezer id. Among this information when it is available a url to listen to a free sample is provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request, json \n",
    "def gettrackinfo(number):\n",
    "    track_url =  \"https://api.deezer.com/track/{}\".format(tracks_list_ordered[number].split(\"_\")[1])\n",
    "    with urllib.request.urlopen(track_url) as url:\n",
    "        data = json.loads(url.read().decode())\n",
    "    return data\n",
    "track_apidata = gettrackinfo(find_track(\"Hexagone\"))\n",
    "track_apidata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can use it to listen a preview:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Audio, clear_output\n",
    "display(Audio(track_apidata[\"preview\"],autoplay=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">Create a radio function that takes as input a track number in the dataset and launches a series of nb_track tracks by randomly pulling in the neighborhood of the current track the next track to listen to. The size of the neighborhood will be configurable and you will delete from the proposals the songs already listened to. You will handle exceptions if the track does not have an available extract. You can delete the current song with the clear_display function.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def start_radio(seed,nb_candidates,duration,nbsteps=20):\n",
    "    print(meta.loc[seed,\"title\"])\n",
    "    display(Audio(meta.loc[seed,\"preview\"],autoplay=True))\n",
    "    time.sleep(duration)\n",
    "    clear_output()\n",
    "    already_played = [seed]\n",
    "    for i in range(nbsteps):\n",
    "        try:\n",
    "            # TODO\n",
    "        except:\n",
    "            print(\"track not found\")\n",
    "            pass\n",
    "        clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_radio(find_track(\"Hexagone\"),5,5,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
